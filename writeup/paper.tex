\documentclass[10pt, conference, compsocconf]{IEEEtran}

%% Custom TeX

\usepackage{listings}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{amssymb}
\usepackage{url}
\usepackage[splitrule]{footmisc}

\usepackage[dvipsnames]{xcolor}

\usepackage{listings}
\lstloadlanguages{Ruby,Java}
\lstset{
basicstyle=\ttfamily\color{black},
commentstyle = \ttfamily\color{red},
keywordstyle=\ttfamily\color{blue},
stringstyle=\color{orange}}

%% for comments
\newcommand{\comment}[3][\color{red}]{{#1{[{#2}: {#3}]}}}
\newcommand{\code}[1]{\textsf{\small #1}}
\newcommand{\kris}[1]{\comment[\color{orange}]{km}{#1}}

\newcommand{\thickhline}{\noalign{\hrule height 1pt}}

%% End of custom TeX

\begin{document}
%
% --- Author Metadata here ---
% --- End of Author Metadata ---

\title{Using Big Data for Music Synthesis}

%%\numberofauthors{3}
\author{
\IEEEauthorblockN{Christopher Imbriano, Amanda Strickler, Kristopher Micinski}
\IEEEauthorblockA{Computer Science Department, University of Maryland,
  College Park, MD 20742, USA
}
}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The big data revolution has revolutionized the way we think about,
design, and implement algorithms at scale.  As such, problems from all
different areas of computer science have been lifted into the
MapReduce framework.  By contrast, MapReduce has been used relatively
less to explore art or creative tasks.  Our project looks to answer:
what kinds of art can we create harnessing large amounts of data at
scale?

We present a system to synthesize music using large amounts of data.
The input is a large corpus of images, which is subsequently processed
by our system, and the result is a musical score.  We then mix and
listen to the music.

\subsection{Motivation} 

We are currently unaware of any projects using MapReduce to create art
or any other representation of digital media.  Because of this, we
anticipate that there might be problems in handling media (buffering,
representing, etc..).  We anticipate that exploring this area might
open up potential applications.

\section{Overview}
\label{sec:overview}

The core idea in converting image data to music is to view the
information in an image (color, size, opacity, etc..) as a mapping to
notes and durations with which those notes are sustained.  In our
system we chose MIDI as the output format.  Our system can then be
modeled as a function \kris{transducer?} from image data to MIDI
output.  Note that the choice of MIDI limits the expressivity of our
music.  Going to other formats (e.g., MP3, OOG, etc..) could provide a
larger range of primitives with which to construct music, but does not
enjoy the simplicity that MIDI provides.

Our system is modeled as a function from ordered sets of images to a
MIDI file.  In our view, a MIDI file consists of note and duration
pairs.  Taken together, these constitute a score by chaining together
multiple pairs over the course of time:

\[
MIDI : (Note,Duration)^{\ast}
\]

%% \[
%%     f : image^{\ast} \to (
%% \]

\subsection{The HSV format}

To convert image data to musical notes, we need to establish a
bijection between the data in the image and the range of music in the
song.  The way we do this is inspired by viewing colors in images as
HSV values.  In HSV, each color has three components: a hue, a
saturation, and a value.  These components are visualized (as shown in
Figure~\ref{fig:hsv}) using a wheel.  The components represent:

\begin{enumerate}
\item Hue --- The actual ``color'' that would be described.  At full
  saturation and value, the hue determines the color seen.  This is
  visualized as going around a wheel of colors.

\item Saturation --- The degree to which the color is saturated from
  white.  Zero saturation is a blank (white) surface, while increasing
  values show more permeance of color.

\item Value --- The amount of light in the color.  Colors with low
  values are dark, and those with higher values are brighter.  This is
  visualized by going up the cylinder.
\end{enumerate}

\kris{Here I'm going to draw a diagram which shows two points on the
  HSV wheel and a musical clef, illustrating the position.}

In our transformation to music, we map colors in HSV to notes in MIDI.
To do this, we map colors in RGB to HSV using the \code{\#RGBtoHSV}
method of the \code{java.awt.Color} class.

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{hsv.png}
  \caption{The HSV color cylinder}
  \label{fig:hsv}
\end{figure}

\subsection{Generating music}
\label{sec:generating}

To generate music, we take each image in our corpus and view it as a
single step in time.  Musical scores are made by streaming large
amounts of images.  (In fact, one possible idea is to use image data
from subsequent frames in movies to get streams of related images.)
Because our music would be relatively uninteresting if it had only one
``player'', we also include several \emph{regions} in the generated
music.  Each region is a distinct musical voice that can play
different notes and is mixed together with the rest of the music.  To
state this formally, each image is mapped to a set of regions and
notes:

\[
    f : A_{i,j} \to note^n
\]

where $i$ and $j$ are the dimensions of the image, and $n$ is the
number of distinct regions of the image.  The obvious idea is to
divide the image into $n$ distinct quadrants of equal size and project
the colors in that region to notes.  One problem is that we often deal
with relatively large images: if we have a smaller set of quadrants
how do we take this to a HSV value?  We solve this problem by taking
thumbnails of images, essentially doing a projection from $A_{i,j}$
down to $A_{n^{.5},n^{.5}}$.  This also tacitly stipulates that we
start out with square images (otherwise we lose some of the image data
in the projection): we handle this step as a first preprocessing phase
where we take images and project them to square images.

In the case of MIDI, notes are pairs of integers in the range
$[-1,127]$.  The first element of the pair represents the \emph{tone}
, and the second element of the pair represents the \emph{velocity}
(how loud the note is):

\[
    note : tone \times velocity
\]

where a velocity of $-1$ represents the lack of any tone, and $127$
represents the loudest possible tone.  Colors start at $C_0$ and move
up over ten octaves.

We represent each region of the image by a representative color, which
is obtained (as noted previously) using thumbnailing.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{image1}
  \caption{A subfigure}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{image1}
  \caption{A subfigure}
  \label{fig:sub2}
\end{subfigure}
\caption{A figure with two subfigures}
\label{fig:test}
\end{figure}

\section{Implementation}

\begin{figure*}
  \centering
  \includegraphics[width=.85\textwidth]{pipeline.png}
  \caption{High level view of the pipeline used for the project}
  \label{fig:project-pipeline}
\end{figure*}

Our implementation consists of scripts to transform the input,
MapReduce phases, and a music mixing tool.  The high level pipeline is
shown in Figure~\ref{fig:project-pipeline}.

\paragraph{Conversion of images to thumbnails}

Conversion to music happens by doing operations on colors and image
data.  To do the conversion to tone, velocity pairs, we transform
colors in the input data, meaning that we have to start with a
representation of $R$ colors for each image.  To do this
transformation, we considered using a MapReduce job: mapping over the
sets of images in our input corpus.  Ultimately we found that
converting images to thumbnails was better accomplished using scripts:
which allowed us to do other manipulation as well.  We anticipate that
if this became a problem we could have moved the image processing code
to Java and pass in our input corpus to the initial stage of
MapReduce.  Our current implementation uses the RMagick \cite{rmagick}
gem, which is a Ruby library interfacing to ImageMagick
\cite{imagemagick}.  ImageMagick includes facilities for taking images
to thumbnails.

\begin{figure}
\begin{lstlisting}[language=Ruby]
require 'rmagick'

class ThumbMaker
  # Convert set of images to thumbnails
  def self.convert_thumbs(dir_path, size)
      # ... 
      puts "Making thumbnail for..."
      Dir.glob("#{path}/*.jpg") do |fname|
        # ...
        fname = # ...
        img = Magick::Image.read(fname)[0]
        thumb = img.thumbnail(size, size)
        thumb.write(thumb_dest)
      end
    end
end

\end{lstlisting}
\caption{Relevant code from the module used for image to thumbnail
  conversion}
\end{figure}

\paragraph{Conversion to intermediate representation}
\label{sec:conversion}

Image data comes in a variety of different formats: JPEG, GIF, PNG,
etc..  Dealing with the plethora of formats presents a problem for our
translation because there are many artifacts of the formats
(annotations, geotagging, compression, etc..) which are unnecessary
for our implementation.  To keep the core abstraction of images as
arrays of bytes (the abstraction described in
Section~\ref{sec:overview}) the first stage of our Hadoop
implementation\footnote{\code{src/main/phase1}} converts the surface
image representation to the sequence file format used in subsequent
processing.  Although it may appear inefficient to hold an
uncompressed image in memory, this is actually ideal for Hadoop:
because computing cycles (incurred through recompressing and sending
the images) are the cost, compared with network traffic.

The conversion in our prototype uses the java \code{ImageBuffer} class
to do image input and conversion.  This class holds requires a large
amount of memory to represent image objects, however this is fine
because we hold only a small number of images in memory at any one
time (using statics) when mapping over our input corpus.

Our implementation for this conversion happens in the
\code{WriteImagesToSequenceFile} class.  They key novelty in this
class is the conversion of image data to a \code{SequenceFile} based
representation.  A \code{SequenceFile} works well for our application
because we need to stream potentially large amounts of image data (raw
bytes).  This is fundamentally no different than creating a custom
Writable instance to serialize to a \code{SequenceFile} at a later
point, but given the simplicity of our representation we say no need
to do this.

\begin{figure}
\begin{lstlisting}[language=Java]
public class WriteImagesToSequenceFile {
  BufferedImage img = null;
  SequenceFile.Writer writer = null;
  FileSystem fs = FileSystem.get(conf);

  FileStatus[] fss = 
    fs.globStatus(new Path(inputPath 
      + "/*.jpg"));

  writer = SequenceFile.createWriter(fs, 
    conf, outPath, IntWritable.class,
    ArrayListOfIntsWritable.class);
  int counter = 0;
  for (FileStatus status : fss) {
    IntWritable image_counter = 
      new IntWritable();
    ArrayListOfIntsWritable 
      pixelData = 
        new ArrayListOfIntsWritable();

    img = ImageIO.read(
      new File(status.getPath()
               .toString().substring(5)));

    // Get the RBG integer for each pixel 
    // in 3X3
    for(int j = 0; j < thumbs; j++) {
      for(int i = 0; i< thumbs; i++) {
        pixelData.add(img.getRGB(i,j));
      }
    }
    
    // Write to the sequence file
    image_counter.set(counter);
    writer.append(image_counter, pixelData);
    counter += 1;
  } 
}
\end{lstlisting}
\caption{Snippet of code from \code{WriteToSequenceFile} showing
  format used in sequence files throughout the MapReduce jobs.}
\end{figure}

\paragraph{Translating to note, region pairs}

After taking the image data to an intermediate format, we must map the
image data to a set of note / region pairs for use in the MIDI.  To do
this, we map a conversion function across each of the different
portions of each image.  The conversion happens by mapping over our
set of images in the \code{Image2MusicMR} class.  The output of the
image to music conversion is $n$ sets of note lists (where notes are
expressed as pairs).  

The strategy used in the conversion is to map over each pixel in an
image, apply the conversion function, and emit it to a reducer to
collate together.  After mapping over each pixel, we need to end up
with a stream of images for each region: a naive implementation simply
maps each pixel in our input corpus to these tone, velocity pairs:

\[
  map : id \times (r,g,b) \to region \times (tone \times velocity)
\]

However, when we actually mix the music, we need each of the streams
to be in an ordered format.  Because the default sorting order does
not take this into account, the music ends up jumbled (each of the
regions is permuted with respect to input ID in an arbitrary way).  To
fix this, we perform secondary sorting on the input ID:

\[
  map : id \times (r,g,b) \to (id \times region) \times (tone \times velocity)
\]



\subsection{Mixing and producing playable music}

The result of our mapreduce pipeline is a set of MIDI music: one for
each region in the image set.  To turn these into music that can be
played, we use mixing software (such as GarageBand) to mix the regions
into full scores.

\section{Results}

\subsection{Sample datasets}

To test the system, we use a variety of different sets of images.
These images were obtained from
Flickr\footnote{\url{http://www.flickr.com/}}, using a public API and
a Python script \cite{flickerpy} to get test data.

\section{Related work}

Many domain specific programming languages exist for music synthesis
[\cite{haskore},\cite{lilypad}].  Many of these languages synthesize
music in the small ().  By contrast, our approach focuses on using
large amounts of data to create music...

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{paper}

\end{document}
